{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f842a13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ajout de la racine du projet au sys.path : /home/tibia/Documents/LLM/BrainGPT/\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "PROJECT_ROOT = \"/home/tibia/Documents/LLM/BrainGPT/\"\n",
    "\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "\n",
    "    sys.path.append(PROJECT_ROOT)\n",
    "    print(f\"Ajout de la racine du projet au sys.path : {PROJECT_ROOT}\")\n",
    "else:\n",
    "    print(\"La racine du projet est déjà dans le sys.path.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bf18c6",
   "metadata": {},
   "source": [
    "## BrainGPT Pipleline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e3daae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ajout du chemin d'évaluation au sys.path : /home/tibia/Documents/LLM/BrainGPT/evaluation/\n",
      "\n",
      "SUCCÈS: 'flamingo' est visible !\n",
      "Flamingo importé depuis : /home/tibia/Documents/LLM/BrainGPT/evaluation/flamingo/__init__.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tibia/Documents/LLM/BrainGPT/braingpt_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Définir le chemin du répertoire 'evaluation' qui contient 'flamingo'\n",
    "EVALUATION_PATH = \"/home/tibia/Documents/LLM/BrainGPT/evaluation/\"\n",
    "\n",
    "# 1. S'assurer que le répertoire 'evaluation/' est dans le chemin de recherche\n",
    "if EVALUATION_PATH not in sys.path:\n",
    "    # 2. Ajout de ce chemin pour que Python puisse résoudre 'from flamingo...'\n",
    "    sys.path.append(EVALUATION_PATH)\n",
    "    print(f\"Ajout du chemin d'évaluation au sys.path : {EVALUATION_PATH}\")\n",
    "\n",
    "# 3. Vérification de l'importation de flamingo\n",
    "try:\n",
    "    import flamingo\n",
    "    print(\"\\nSUCCÈS: 'flamingo' est visible !\")\n",
    "    print(f\"Flamingo importé depuis : {flamingo.__file__}\") \n",
    "except ModuleNotFoundError:\n",
    "    print(\"\\nÉCHEC: 'flamingo' n'est toujours pas visible. Problème de configuration.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "520a5a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-15 17:14:59,240] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-15 17:14:59,559] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tibia/Documents/LLM/BrainGPT/braingpt_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using config.init_device='cpu', but you can also use config.init_device=\"meta\" with Composer + FSDP for fast initialization.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tibia/Documents/LLM/BrainGPT/evaluation/flamingo/mpt/param_init_fns.py:28: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)\n",
      "  init_fn_(module.weight[slice_indices])\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current model version is configured for Otter-Image with max_num_frames set to None.\n",
      "Total Trainable param: 1.385404 B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:18<00:00,  4.71s/it]\n",
      "Some weights of OtterForConditionalGeneration were not initialized from the model checkpoint at /home/tibia/Documents/LLM/BrainGPT/checkpoints/OTTER_CLIP_BRAINGPT_hf/ and are newly initialized: ['vision_encoder.vision_model.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle chargé : <class 'otter.modeling_otter.OtterForConditionalGeneration'>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from otter.modeling_otter import OtterForConditionalGeneration\n",
    "\n",
    "model = OtterForConditionalGeneration.from_pretrained(\n",
    "                \"/home/tibia/Documents/LLM/BrainGPT/checkpoints/OTTER_CLIP_BRAINGPT_hf/\"\n",
    ")\n",
    "\n",
    "# Affichez le modèle pour vérifier qu'il est chargé (facultatif)\n",
    "print(f\"Modèle chargé : {type(model)}\")\n",
    "\n",
    "tokenizer = model.text_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd29086",
   "metadata": {},
   "source": [
    "### Création dataset : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cc76463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Création d'une fausse classe d'arguments pour remplacer argparse braingpt\n",
    "class SimpleArgs:\n",
    "    def __init__(self, tokenizer):\n",
    "        # Paramètres requis par MimicitDataset.__init__\n",
    "        self.tokenizer = tokenizer\n",
    "        self.task = \"inference\"\n",
    "        self.max_src_length = 512      # Valeur par défaut standard\n",
    "        self.max_tgt_length = 512      # Valeur par défaut standard\n",
    "        self.seed = 42                 # Valeur par défaut standard\n",
    "        self.patch_image_size = 224  # Taille standard des patchs image\n",
    "        self.inst_format= \"simple\"\n",
    "        \n",
    "        # Paramètres pour le DataLoader\n",
    "        self.workers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15bb3769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from pipeline.mimicit_utils.mimicit_dataset import MimicitDataset\n",
    "\n",
    "def get_inference_dataloader(args, tokenizer):\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Version simplifiée pour l'inférence avec --mimicit_path et --images_path uniquement.\n",
    "    \"\"\"\n",
    "    # 1. Configuration minimale requise par MimicitDataset\n",
    "    args = SimpleArgs(tokenizer)\n",
    "    \n",
    "    # 2. Préparation des listes de chemins\n",
    "    # MimicitDataset attend des listes, même pour un seul fichier\n",
    "    mimicit_paths = [\"/home/tibia/Documents/LLM/BrainGPT/data/instruction_dataset_3.json\"]\n",
    "    image_paths = [\"/home/tibia/Documents/LLM/BrainGPT/data/Adri.json\"]\n",
    "\n",
    "    # Gestion de la config d'entraînement (probablement vide pour vous, mais requise par la classe)\n",
    "    #  on passe une liste avec une chaîne vide\n",
    "    train_config_paths = [\"\"]\n",
    "    \n",
    "    # Statut des données (tout est \"new\" pour l'inférence)\n",
    "    status_list = [\"new\"]\n",
    "\n",
    "    print(f\"Chargement du dataset depuis : {mimicit_paths[0]} et {image_paths[0]}\")\n",
    "\n",
    "    # 3. Création de l'instance du Dataset\n",
    "    # On suppose que la classe MimicitDataset est importée\n",
    "    dataset = MimicitDataset(\n",
    "        args, \n",
    "        mimicit_paths, \n",
    "        image_paths, \n",
    "        train_config_paths, \n",
    "        status_list=status_list\n",
    "    )\n",
    "\n",
    "    # 4. Création du DataLoader simple\n",
    "    # - batch_size=1 : Pour traiter une image à la fois\n",
    "    # - shuffle=False : IMPORTANT pour l'inférence (garder l'ordre des fichiers)\n",
    "    # - drop_last=False : On veut traiter TOUTES les images, même si le compte n'est pas rond\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=1, \n",
    "        num_workers=args.workers, # ou 1\n",
    "        pin_memory=True,\n",
    "        shuffle=False, \n",
    "        drop_last=False,\n",
    "        collate_fn=dataset.collate # On garde la méthode de formatage du dataset\n",
    "    )\n",
    "\n",
    "    # On retourne directement le dataloader (pas une liste de dataloaders)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8456553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement du dataset depuis : /home/tibia/Documents/LLM/BrainGPT/data/instruction_dataset_3.json et /home/tibia/Documents/LLM/BrainGPT/data/Adri.json\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "device_id = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device_id)\n",
    "batch_mimicit = next(iter(get_inference_dataloader(SimpleArgs(tokenizer), tokenizer)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ceba9a",
   "metadata": {},
   "source": [
    "####  Test sur le batch_mimicit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbd11b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch keys: dict_keys(['id', 'nsentences', 'net_input'])\n",
      "id (valeur): ['INS_ID_00526c11_ID_d6296de728']\n",
      "net_input keys: dict_keys(['input_ids', 'attention_masks', 'patch_images'])\n",
      "nsentences type: <class 'int'>\n",
      "nsentences value: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Batch keys:\", batch_mimicit.keys())\n",
    "\n",
    "# id\n",
    "# On vérifie si c'est une liste avant de demander la longueur, pour être sûr\n",
    "if isinstance(batch_mimicit[\"id\"], (list, tuple)):\n",
    "    print(\"id length:\", len(batch_mimicit[\"id\"]))\n",
    "    print(\"Premier ID:\", batch_mimicit[\"id\"][0]) # Utile pour vérifier\n",
    "else:\n",
    "    print(\"id (valeur):\", batch_mimicit[\"id\"])\n",
    "\n",
    "# net_input\n",
    "print(\"net_input keys:\", batch_mimicit[\"net_input\"].keys())\n",
    "\n",
    "\n",
    "# On affiche la valeur directe et le type\n",
    "print(\"nsentences type:\", type(batch_mimicit[\"nsentences\"]))\n",
    "print(\"nsentences value:\", batch_mimicit[\"nsentences\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6c08a4",
   "metadata": {},
   "source": [
    "### Train_one_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "339e1a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "media_token_id = tokenizer(\"<image>\", add_special_tokens=False)[\"input_ids\"][-1]\n",
    "endofchunk_token_id = tokenizer(\"<|endofchunk|>\", add_special_tokens=False)[\"input_ids\"][-1]\n",
    "answer_token_id = tokenizer(\"<answer>\", add_special_tokens=False)[\"input_ids\"][-1]\n",
    "ens_token_id = tokenizer(tokenizer.eos_token, add_special_tokens=False)[\"input_ids\"][-1]\n",
    "\n",
    "model.eval()\n",
    "\n",
    "\n",
    "images = batch_mimicit[\"net_input\"][\"patch_images\"].to(device_id, non_blocking=True)\n",
    "input_ids = batch_mimicit[\"net_input\"][\"input_ids\"].to(device_id, non_blocking=True)\n",
    "attention_mask = batch_mimicit[\"net_input\"][\"attention_masks\"].to(device_id, non_blocking=True)\n",
    "\n",
    "labels = input_ids.clone()\n",
    "labels[labels == tokenizer.pad_token_id] = -100\n",
    "labels[:, 0] = -100\n",
    "for i in range(labels.shape[0]):\n",
    "                # remove loss for any token before the first <image> token\n",
    "                # label_idx = 0\n",
    "                # while label_idx < labels.shape[1] and labels[i][label_idx] != media_token_id:\n",
    "                #     labels[i][label_idx] = -100\n",
    "                #     label_idx += 1\n",
    "\n",
    "                # <image>User: {cur_incontext_instruction} GPT:<answer> {cur_incontext_answer}<|endofchunk|>User: {instruction} GPT:<answer> {answer}<|endofchunk|>\n",
    "                # <image>User: {cur_incontext_instruction} GPT:<answer> {cur_incontext_answer}<|endofchunk|><image>User: {instruction} GPT:<answer> {answer}<|endofchunk|>\n",
    "\n",
    "                # get index of all endofchunk/media tokens in the sequence\n",
    "    endofchunk_idxs = torch.where(labels[i] == endofchunk_token_id)[0]\n",
    "    media_idxs = torch.where(labels[i] == media_token_id)[0]\n",
    "\n",
    "                # remove loss for any token the before the first <answer>\n",
    "    token_idx = 0\n",
    "    while token_idx < labels.shape[1] and labels[i][token_idx] != answer_token_id:\n",
    "        labels[i][token_idx] = -100\n",
    "        token_idx += 1\n",
    "\n",
    "                # remove loss for any token between <|endofchunk|> and <answer>, except <image>\n",
    "        for endofchunk_idx in endofchunk_idxs[:-1]:\n",
    "            token_idx = endofchunk_idx + 1\n",
    "            while token_idx < labels.shape[1] and labels[i][token_idx] != answer_token_id:\n",
    "                if labels[i][token_idx] == media_token_id:\n",
    "                    pass\n",
    "                else:\n",
    "                    labels[i][token_idx] = -100\n",
    "                    token_idx += 1\n",
    "\n",
    "labels[labels == answer_token_id] = -100\n",
    "labels[labels == media_token_id] = -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad5c3c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " lang x : {'input_ids': tensor([[50278,  6989,    27,  3666, 19268,   253,  2460,   275,   247,  2014,\n",
      "          6197,    15,   443,  5736,    27, 50279]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# images ??\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "dtype = model.dtype\n",
    "\n",
    "test_prompt = \"Describe the image in a single sentence.\"\n",
    "lang_x = model.text_tokenizer(\n",
    "                    [f\"<image>User: {test_prompt} GPT:<answer>\"],\n",
    "                    return_tensors=\"pt\",\n",
    "                )\n",
    "print(f\" lang x : {lang_x}\")\n",
    "\n",
    "lang_x_input_ids = lang_x[\"input_ids\"].to(device)\n",
    "lang_x_attention_mask = lang_x[\"attention_mask\"].to(device)\n",
    "\n",
    "\n",
    "\n",
    "generated_text = model.generate(\n",
    "                    vision_x=images.to(dtype),\n",
    "                    lang_x=lang_x_input_ids,\n",
    "                    attention_mask=lang_x_attention_mask,\n",
    "                    max_new_tokens = 512,\n",
    "                ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf59e29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              id gt  \\\n",
      "0  INS_ID_00526c11_ID_d6296de728      \n",
      "\n",
      "                                       parsed_output  \n",
      "0  > Mild generalized atrophic change of the brai...  \n"
     ]
    }
   ],
   "source": [
    "# géneration du texte \n",
    "import pandas as pd\n",
    "\n",
    "generated_captions = {}\n",
    "\n",
    "\n",
    "parsed_output = (\n",
    "                    model.text_tokenizer.decode(generated_text[0])\n",
    "                    .split(\"<answer>\")[-1]\n",
    "                    .lstrip()\n",
    "                    .rstrip()\n",
    "                    .split(\"<|endofchunk|>\")[0]\n",
    "                    .lstrip()\n",
    "                    .rstrip()\n",
    "                    .lstrip('\"')\n",
    "                    .rstrip('\"')\n",
    "                )\n",
    "gt = (\n",
    "                    model.text_tokenizer.decode(input_ids[0])\n",
    "                    .split(\"<answer>\")[-1]\n",
    "                    .lstrip()\n",
    "                    .rstrip()\n",
    "                    .split(\"<|endofchunk|>\")[0]\n",
    "                    .lstrip()\n",
    "                    .rstrip()\n",
    "                    .lstrip('\"')\n",
    "                    .rstrip('\"')\n",
    "                )\n",
    "                # print(batch_mimicit.keys())\n",
    "#                 print(\"/\",parsed_output,\"/\")\n",
    "generated_captions[batch_mimicit[\"id\"][0]] = (gt, parsed_output)\n",
    "                # print(generated_captions.keys())\n",
    "\n",
    "    # print(generated_captions)\n",
    "df_data = [(key, val[0], val[1]) for key, val in generated_captions.items()]\n",
    "df = pd.DataFrame(df_data, columns=['id', 'gt', 'parsed_output'])\n",
    "print(df)\n",
    "\n",
    "df.to_csv(\"/home/tibia/Documents/LLM/BrainGPT/notebooks/output2\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1d2c55",
   "metadata": {},
   "source": [
    "### Test tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1f562f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Attriut du modèle pour vérifier qu'il est bien chargé\n",
    "print(f\"Nombre de paramètres du modèle : {sum(p.numel() for p in model.parameters())}\")\n",
    "print(f\"tokenizer du modèle : {model.text_tokenizer} \")\n",
    "\n",
    "\n",
    "# analyse tokenizer \n",
    "\n",
    "tokenizer= model.text_tokenizer\n",
    "\n",
    "print(\"--- Type du Tokenizer ---\")\n",
    "print(type(tokenizer))\n",
    "\n",
    "\n",
    "print(\"\\n--- Configuration et Vocabulaire ---\")\n",
    "print(f\"Taille du vocabulaire : {len(tokenizer)}\")\n",
    "print(f\"Token de fin de séquence (EOS) : {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})\")\n",
    "print(f\"Token de padding (PAD) : {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
    "\n",
    "# 3. Test simple  ?\n",
    "test_phrase = \"how otter deal with phrases i give him ?\"\n",
    "print(f\"\\n--- Test de Tokenisation de : '{test_phrase}' ---\")\n",
    "tokens = tokenizer(test_phrase, return_tensors='pt')\n",
    "print (f\"Tokens : {tokens}\")\n",
    "print(f\"Tokens ID : {tokens['input_ids'][0].tolist()}\") # O pour déballerr tensor\n",
    "print(f\"Tokens décodés : {tokenizer.convert_ids_to_tokens(tokens['input_ids'][0].tolist())}\")\n",
    "\n",
    "\n",
    "#test decodage \n",
    "tokenizer.decode(tokens[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3c963b",
   "metadata": {},
   "source": [
    "## Hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "803ad601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Using', 'Ġa', 'ĠTrans', 'former', 'Ġnetwork', 'Ġis', 'Ġsimple']\n",
      "[11888, 247, 4480, 19946, 2990, 310, 2969]\n"
     ]
    }
   ],
   "source": [
    "sequence = \"Using a Transformer network is simple\"\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "\n",
    "\n",
    "print(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcc911c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bbc14a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Using', 'Ġa', 'ĠTrans', 'former', 'Ġnetwork', 'Ġis', 'Ġsimple']\n",
      "[7993, 170, 13809, 23763, 2443, 1110, 3014]\n",
      "Using a Transformer network is simple\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tibia/Documents/LLM/BrainGPT/braingpt_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer_2 = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "sequence = \"Using a Transformer network is simple\"\n",
    "tokens_2 = tokenizer_2.tokenize(sequence)\n",
    "\n",
    "print(tokens)\n",
    "ids = tokenizer_2.convert_tokens_to_ids(tokens_2)\n",
    "\n",
    "print(ids)\n",
    "decoded_string = tokenizer_2.decode(ids)\n",
    "print(decoded_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f298bcc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
      "          2026,  2878,  2166,  1012]])\n",
      "Input IDs 2: tensor([[1045, 5223, 2023, 2061, 2172]])\n",
      "Logits: tensor([[-2.7276,  2.8789]], grad_fn=<AddmmBackward0>)\n",
      "Logits 2: tensor([[ 3.1744, -2.6848]], grad_fn=<AddmmBackward0>)\n",
      "Batched Input IDs: tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
      "          2026,  2878,  2166,  1012],\n",
      "        [ 1045,  5223,  2023,  2061,  2172,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0]])\n",
      "Batched Logits: tensor([[-2.7276,  2.8789],\n",
      "        [ 3.1744, -2.6848]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "sequence_2 = \"I hate this so much \"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "tokens_2 = tokenizer.tokenize(sequence_2)\n",
    "ids_2 = tokenizer.convert_tokens_to_ids(tokens_2)\n",
    "\n",
    "\n",
    "\n",
    "input_ids = torch.tensor([ids])\n",
    "print(\"Input IDs:\", input_ids)\n",
    "input_ids_2 = torch.tensor([ids_2])\n",
    "print(\"Input IDs 2:\", input_ids_2)\n",
    "\n",
    "output = model(input_ids)\n",
    "print(\"Logits:\", output.logits)\n",
    "\n",
    "output_2 = model(input_ids_2)\n",
    "print(\"Logits 2:\", output_2.logits)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pad_length = len(ids) - len(ids_2)\n",
    "# Crée une liste contenant 'pad_token_id' répété 'pad_length' fois\n",
    "padding_tokens = [tokenizer.pad_token_id] * pad_length\n",
    "\n",
    "mask_1 = [1] * len(ids) \n",
    "mask_2 = ([1] * len(ids_2)) + ([0] * pad_length) \n",
    "attention_mask = [mask_1, mask_2]\n",
    "\n",
    "batched_ids = [ids, ids_2 + padding_tokens]\n",
    "input_ids = torch.tensor(batched_ids)\n",
    "\n",
    "print(\"Batched Input IDs:\", input_ids)\n",
    "output = model(input_ids, attention_mask=torch.tensor([attention_mask]))\n",
    "print(\"Batched Logits:\", output.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e2cb3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tibia/Documents/LLM/BrainGPT/braingpt_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 1.5694, -1.3895],\n",
      "        [ 1.3374, -1.2163]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 1.5694, -1.3895],\n",
      "        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence1_ids = [[200, 200, 200]]\n",
    "sequence2_ids = [[200, 200]]\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "# problème ici \n",
    "print(model(torch.tensor(sequence1_ids)).logits)\n",
    "print(model(torch.tensor(sequence2_ids)).logits)\n",
    "print(model(torch.tensor(batched_ids)).logits)\n",
    "\n",
    "\n",
    "attention_mask = [\n",
    "    [1, 1, 1],\n",
    "    [1, 1, 0],\n",
    "]\n",
    "\n",
    "outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))\n",
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02897e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tibia/Documents/LLM/BrainGPT/braingpt_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/tibia/Documents/LLM/BrainGPT/braingpt_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-15 12:11:07,253] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-15 12:11:07,577] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tibia/Documents/LLM/BrainGPT/braingpt_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face est une entreprise qui promeut le nouvelles, les corsages du fait sur deux.\n",
      "(1) In this passage there is a slight difference between the French and English translations of \"The Little Ghost\" (1704), where one character has its\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 1. Charger le modèle et le tokenizer\n",
    "model_name = \"gpt2\" # Exemple\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# 2. Préparer l'entrée\n",
    "prompt = \"Hugging Face est une entreprise qui promeut\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\") # Convertir le texte en tenseurs\n",
    "\n",
    "# 3. Générer le texte\n",
    "generated_ids = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=50,       # Max 50 nouveaux jetons\n",
    "    do_sample=True,          # Utiliser l'échantillonnage\n",
    "    top_p=0.95,              # Filtrer avec Top-P\n",
    "    temperature=0.7,         # Contrôler l'aléa\n",
    "    repetition_penalty=1.2,  # Éviter la répétition\n",
    ")\n",
    "\n",
    "# 4. Décoder le résultat\n",
    "output = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd63554a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tibia/Documents/LLM/BrainGPT/braingpt_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f shape en sortie du modèle : torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "output = model(**tokens)\n",
    "print(\"f shape en sortie du modèle :\", output.logits.shape)  # Devrait être (2, 2) pour deux séquences et deux classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d98866ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Type du Processeur d'Image ---\n",
      "<class 'transformers.models.clip.image_processing_clip.CLIPImageProcessor'>\n",
      "\n",
      "--- Paramètres de Prétraitement ---\n",
      "Taille de redimensionnement (resize) : {'shortest_edge': 224}\n",
      "Taille de recadrage (crop size) : {'height': 224, 'width': 224}\n",
      "Moyenne (mean) utilisée pour la normalisation : [0.48145466, 0.4578275, 0.40821073]\n",
      "Écart-type (std) utilisé pour la normalisation : [0.26862954, 0.26130258, 0.27577711]\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPImageProcessor\n",
    "\n",
    "image_processor = CLIPImageProcessor()\n",
    "print(\"--- Type du Processeur d'Image ---\")\n",
    "print(type(image_processor))\n",
    "\n",
    "# 2. Afficher les paramètres de prétraitement (très importants)\n",
    "print(\"\\n--- Paramètres de Prétraitement ---\")\n",
    "print(f\"Taille de redimensionnement (resize) : {image_processor.size}\")\n",
    "print(f\"Taille de recadrage (crop size) : {image_processor.crop_size}\")\n",
    "print(f\"Moyenne (mean) utilisée pour la normalisation : {image_processor.image_mean}\")\n",
    "print(f\"Écart-type (std) utilisé pour la normalisation : {image_processor.image_std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aee30f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 8667, 117, 146, 112, 182, 170, 1423, 5650, 106, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['Jim', 'Henson', 'was', 'a', 'puppeteer']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "encoded_input = tokenizer(\"Hello, I'm a single sentence!\")\n",
    "print(encoded_input)\n",
    "\n",
    "\n",
    "tokenized_text = \"Jim Henson was a puppeteer\".split()\n",
    "print(tokenized_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "braingpt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
