{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f842a13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La racine du projet est déjà dans le sys.path.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "PROJECT_ROOT = \"/home/tibia/Documents/LLM/BrainGPT/\"\n",
    "\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "\n",
    "    sys.path.append(PROJECT_ROOT)\n",
    "    print(f\"Ajout de la racine du projet au sys.path : {PROJECT_ROOT}\")\n",
    "else:\n",
    "    print(\"La racine du projet est déjà dans le sys.path.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e3daae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ajout du chemin d'évaluation au sys.path : /home/tibia/Documents/LLM/BrainGPT/evaluation/\n",
      "\n",
      "SUCCÈS: 'flamingo' est visible !\n",
      "Flamingo importé depuis : /home/tibia/Documents/LLM/BrainGPT/evaluation/flamingo/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Définir le chemin du répertoire 'evaluation' qui contient 'flamingo'\n",
    "EVALUATION_PATH = \"/home/tibia/Documents/LLM/BrainGPT/evaluation/\"\n",
    "\n",
    "# 1. S'assurer que le répertoire 'evaluation/' est dans le chemin de recherche\n",
    "if EVALUATION_PATH not in sys.path:\n",
    "    # 2. Ajout de ce chemin pour que Python puisse résoudre 'from flamingo...'\n",
    "    sys.path.append(EVALUATION_PATH)\n",
    "    print(f\"Ajout du chemin d'évaluation au sys.path : {EVALUATION_PATH}\")\n",
    "\n",
    "# 3. Vérification de l'importation de flamingo\n",
    "try:\n",
    "    import flamingo\n",
    "    print(\"\\nSUCCÈS: 'flamingo' est visible !\")\n",
    "    print(f\"Flamingo importé depuis : {flamingo.__file__}\") \n",
    "except ModuleNotFoundError:\n",
    "    print(\"\\nÉCHEC: 'flamingo' n'est toujours pas visible. Problème de configuration.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "520a5a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tibia/Documents/LLM/BrainGPT/braingpt_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using config.init_device='cpu', but you can also use config.init_device=\"meta\" with Composer + FSDP for fast initialization.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tibia/Documents/LLM/BrainGPT/evaluation/flamingo/mpt/param_init_fns.py:28: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)\n",
      "  init_fn_(module.weight[slice_indices])\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current model version is configured for Otter-Image with max_num_frames set to None.\n",
      "Total Trainable param: 1.385404 B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:19<00:00,  4.77s/it]\n",
      "Some weights of OtterForConditionalGeneration were not initialized from the model checkpoint at /home/tibia/Documents/LLM/BrainGPT/checkpoints/OTTER_CLIP_BRAINGPT_hf/ and are newly initialized: ['vision_encoder.vision_model.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle chargé : <class 'evaluation.otter.modeling_otter.OtterForConditionalGeneration'>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Ceci était la ligne qui échouait, mais elle devrait fonctionner maintenant\n",
    "from evaluation.otter.modeling_otter import OtterForConditionalGeneration\n",
    "\n",
    "model = OtterForConditionalGeneration.from_pretrained(\n",
    "                \"/home/tibia/Documents/LLM/BrainGPT/checkpoints/OTTER_CLIP_BRAINGPT_hf/\"\n",
    ")\n",
    "\n",
    "# Affichez le modèle pour vérifier qu'il est chargé (facultatif)\n",
    "print(f\"Modèle chargé : {type(model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ccc05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de paramètres du modèle : 8131301392\n",
      "tokenizer du modèle : GPTNeoXTokenizerFast(name_or_path='mosaicml/mpt-7b-instruct', vocab_size=50254, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<PAD>', 'additional_special_tokens': ['<|endofchunk|>', '<image>', '<answer>']}, clean_up_tokenization_spaces=True) \n"
     ]
    }
   ],
   "source": [
    "## Attriut du modèle pour vérifier qu'il est bien chargé\n",
    "print(f\"Nombre de paramètres du modèle : {sum(p.numel() for p in model.parameters())}\")\n",
    "print(f\"tokenizer du modèle : {model.text_tokenizer} \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e63bd29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Type du Tokenizer ---\n",
      "<class 'transformers.models.gpt_neox.tokenization_gpt_neox_fast.GPTNeoXTokenizerFast'>\n",
      "\n",
      "--- Configuration et Vocabulaire ---\n",
      "Taille du vocabulaire : 50281\n",
      "Token de fin de séquence (EOS) : <|endoftext|> (ID: 0)\n",
      "Token de padding (PAD) : <PAD> (ID: 50280)\n",
      "\n",
      "--- Test de Tokenisation de : 'how otter deal with phrases i give him ?' ---\n",
      "Tokens ID : [5430, 14366, 350, 2968, 342, 25491, 891, 1918, 779, 3736]\n",
      "Tokens décodés : ['how', 'Ġot', 'ter', 'Ġdeal', 'Ġwith', 'Ġphrases', 'Ġi', 'Ġgive', 'Ġhim', 'Ġ?']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'how otter deal with phrases i give him?'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# analyse tokenizer \n",
    "tokenizer= model.text_tokenizer\n",
    "\n",
    "print(\"--- Type du Tokenizer ---\")\n",
    "print(type(tokenizer))\n",
    "\n",
    "\n",
    "print(\"\\n--- Configuration et Vocabulaire ---\")\n",
    "print(f\"Taille du vocabulaire : {len(tokenizer)}\")\n",
    "print(f\"Token de fin de séquence (EOS) : {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})\")\n",
    "print(f\"Token de padding (PAD) : {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
    "\n",
    "# 3. Test simple  ?\n",
    "test_phrase = \"how otter deal with phrases i give him ?\"\n",
    "print(f\"\\n--- Test de Tokenisation de : '{test_phrase}' ---\")\n",
    "tokens = tokenizer(test_phrase, return_tensors='pt')\n",
    "print(f\"Tokens ID : {tokens['input_ids'][0].tolist()}\") # O pour déballerr tensor\n",
    "print(f\"Tokens décodés : {tokenizer.convert_ids_to_tokens(tokens['input_ids'][0].tolist())}\")\n",
    "\n",
    "\n",
    "#test decodage \n",
    "tokenizer.decode(tokens[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aee30f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 8667, 117, 146, 112, 182, 170, 1423, 5650, 106, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "encoded_input = tokenizer(\"Hello, I'm a single sentence!\")\n",
    "print(encoded_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "braingpt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
